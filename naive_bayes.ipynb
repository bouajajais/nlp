{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1600860719902",
   "display_name": "Python 3.6.9 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from wordcloud import STOPWORDS\n",
    "from sklearn.model_selection import train_test_split\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classificator:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.vocab = set()\n",
    "        self.word_count = {\n",
    "            'spam': {},\n",
    "            'ham': {}\n",
    "        }\n",
    "        self.freq = {\n",
    "            'spam': {},\n",
    "            'ham': {}\n",
    "        }\n",
    "    \n",
    "    @classmethod\n",
    "    def get_vocab(cls, X):\n",
    "        vocab = map(lambda x: set(x), X)\n",
    "        vocab = reduce(lambda x1, x2: x1.union(x2), vocab)\n",
    "        return vocab\n",
    "    \n",
    "    @classmethod\n",
    "    def word_count(cls, X, Y, count=None):\n",
    "        if count is None:\n",
    "            count = {\n",
    "                'spam': {},\n",
    "                'ham': {}\n",
    "            }\n",
    "        for x, y in zip(X, Y):\n",
    "            for word in x:\n",
    "                if not word in count[y].keys():\n",
    "                    count[y][word] = 1\n",
    "                else:\n",
    "                    count[y][word] += 1\n",
    "        return count\n",
    "    \n",
    "    def word_freq(self, count, vocab):\n",
    "        freq = {\n",
    "            'spam': {},\n",
    "            'ham': {}\n",
    "        }\n",
    "        Nclass = {\n",
    "            'spam': reduce(lambda w1, w2: w1 + w2, count['spam'].values()),\n",
    "            'ham': reduce(lambda w1, w2: w1 + w2, count['ham'].values())\n",
    "        }\n",
    "        V = len(vocab)\n",
    "        for y, ycount in count.items():\n",
    "            for word, _count in ycount.items():\n",
    "                freq[y][word] = (_count + 1) / (Nclass[y] + V)\n",
    "        return freq\n",
    "    \n",
    "    def train(self, X, Y):\n",
    "        self.vocab.update(self.get_vocab(X))\n",
    "        self.word_count(X, Y, self.count)\n",
    "        self.freq = self.word_freq(self.count, self.vocab)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        single = isinstance(X[0], str)\n",
    "        if single:\n",
    "            X = [X]\n",
    "        Y = []\n",
    "        for x in X:\n",
    "            y_value = 1\n",
    "            for word in x:\n",
    "                y_value *= self.freq['spam'][word] / self.freq['ham'][word]\n",
    "            Y.append(y_value)\n",
    "        if single:\n",
    "            Y = Y[0]\n",
    "        return Y\n",
    "    \n",
    "    def test(self, X, Y):\n",
    "        Y_predicted = self.predict(X)\n",
    "\n",
    "    @classmethod\n",
    "    def accuracy(cls, Y, Y_predicted):\n",
    "        pass\n",
    "\n",
    "    @classmethod\n",
    "    def recall(cls, Y, Y_predicted):\n",
    "        pass\n",
    "\n",
    "    @classmethod\n",
    "    def precision(cls, Y, Y_predicted):\n",
    "        pass\n",
    "\n",
    "    @classmethod\n",
    "    def f1(cls, Y, Y_predicted):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.classificator = Classificator()\n",
    "    \n",
    "    @classmethod\n",
    "    def load(filename):\n",
    "        msgs = {\n",
    "            'data' : [],\n",
    "            'target' : []\n",
    "        }\n",
    "\n",
    "        with open(filename) as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                target, msg = line.split('\\t')\n",
    "                msgs['data'].append(msg)\n",
    "                msgs['target'].append(target)\n",
    "        \n",
    "        return msgs\n",
    "\n",
    "    @classmethod\n",
    "    def preprocess(cls, X):\n",
    "        # lowercase (str -> str)\n",
    "        X = [line.lower() for line in X]\n",
    "\n",
    "        # numbers (str -> str)\n",
    "        # X = [re.sub(r'\\d+', '', line) for line in X]\n",
    "\n",
    "        # punctuation (str -> str)\n",
    "        X = [line.translate(str.maketrans(\"\", \"\", string.punctuation)) for line in X]\n",
    "\n",
    "        #replace ’ with ' and remove “ and ” (str -> str)\n",
    "        X = [line.replace(\"’\",\"'\") for line in X]\n",
    "        X = [line.replace(\"“\",\"\") for line in X]\n",
    "        X = [line.replace(\"”\",\"\") for line in X]\n",
    "\n",
    "        # whitespaces (str -> str)\n",
    "        X = [line.strip() for line in X]\n",
    "\n",
    "        # tokenization (str -> list)\n",
    "        X = [line.split(' ') for line in X]\n",
    "\n",
    "        # stopwords (list -> list)\n",
    "        stopwords = set(STOPWORDS)\n",
    "        stopwords.update({})\n",
    "        X = [[word for word in line if len(word) > 2 and not word in stopwords] for line in X]\n",
    "\n",
    "        # Lemmatization (list -> list)\n",
    "        X = [[WordNetLemmatizer().lemmatize(word) for word in line] for line in X]\n",
    "\n",
    "        return X\n",
    "    \n",
    "    @classmethod\n",
    "    def split(cls, data, target, test_size=0.3, random_state=109):\n",
    "        # Split dataset into training set and test set\n",
    "        # 70% training and 30% test (default)\n",
    "        return train_test_split(data, target, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    def train_test(self, filename=\"SMSSpamCollection\", test_size=0.3, random_state=109):\n",
    "        msgs = self.load(filename)\n",
    "        msgs = self.preprocess(msgs['data'])\n",
    "        X_train, X_test, Y_train, Y_test = self.split(msgs['data'], msgs['target'], test_size, random_state)\n",
    "        self.classificator.train(X_train, Y_train)\n",
    "        return self.classificator.test(X_test, Y_test)"
   ]
  }
 ]
}