{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1600860719902",
   "display_name": "Python 3.6.9 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from wordcloud import STOPWORDS\n",
    "from sklearn.model_selection import train_test_split\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classificator:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.vocab = set()\n",
    "        self.count = {\n",
    "            'spam': {},\n",
    "            'ham': {}\n",
    "        }\n",
    "        self.freq = {\n",
    "            'spam': {},\n",
    "            'ham': {}\n",
    "        }\n",
    "    \n",
    "    @classmethod\n",
    "    def get_vocab(cls, X):\n",
    "        vocab = map(lambda x: set(x), X)\n",
    "        vocab = reduce(lambda x1, x2: x1.union(x2), vocab)\n",
    "        return vocab\n",
    "    \n",
    "    @classmethod\n",
    "    def word_count(cls, X, Y, count=None):\n",
    "        if count is None:\n",
    "            count = {\n",
    "                'spam': {},\n",
    "                'ham': {}\n",
    "            }\n",
    "        for x, y in zip(X, Y):\n",
    "            for word in x:\n",
    "                if not word in count[y].keys():\n",
    "                    count[y][word] = 1\n",
    "                else:\n",
    "                    count[y][word] += 1\n",
    "        return count\n",
    "    \n",
    "    def word_freq(self, count, vocab):\n",
    "        freq = {\n",
    "            'spam': {},\n",
    "            'ham': {}\n",
    "        }\n",
    "        Nclass = {\n",
    "            'spam': reduce(lambda w1, w2: w1 + w2, count['spam'].values()),\n",
    "            'ham': reduce(lambda w1, w2: w1 + w2, count['ham'].values())\n",
    "        }\n",
    "        V = len(vocab)\n",
    "        for y, ycount in count.items():\n",
    "            for word, _count in ycount.items():\n",
    "                freq[y][word] = (_count + 1) / (Nclass[y] + V)\n",
    "        return freq\n",
    "    \n",
    "    def train(self, X, Y):\n",
    "        self.vocab.update(self.get_vocab(X))\n",
    "        self.word_count(X, Y, self.count)\n",
    "        self.freq = self.word_freq(self.count, self.vocab)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        assert isinstance(X, list) and len(X) != 0\n",
    "        single = isinstance(X[0], str)\n",
    "        if single:\n",
    "            X = [X]\n",
    "        Y = []\n",
    "        for x in X:\n",
    "            y_value = 1\n",
    "            for word in x:\n",
    "                if word in self.freq['spam'].keys() and word in self.freq['ham'].keys():\n",
    "                    y_value *= self.freq['spam'][word] / self.freq['ham'][word]\n",
    "            y_value = 'spam' if y_value > 1 else 'ham'\n",
    "            Y.append(y_value)\n",
    "        if single:\n",
    "            Y = Y[0]\n",
    "        return Y\n",
    "    \n",
    "    def test(self, X, Y):\n",
    "        pos = 'spam'\n",
    "        neg = 'ham'\n",
    "        Y_predicted = self.predict(X)\n",
    "        TP, TN, FP, FN = 0, 0, 0, 0\n",
    "        for y, y_p in zip(Y, Y_predicted):\n",
    "            if y_p == pos:\n",
    "                if y_p == y: TP += 1\n",
    "                else: FP += 1\n",
    "            else:\n",
    "                if y_p == y: TN += 1\n",
    "                else: FN += 1\n",
    "        accuracy = self.accuracy(TP, TN, FP, FN)\n",
    "        recall = self.recall(TP, FN)\n",
    "        precision = self.precision(TP, FP)\n",
    "        f1 = self.f1(recall, precision)\n",
    "        return accuracy, recall, precision, f1\n",
    "\n",
    "    @classmethod\n",
    "    def accuracy(cls, TP, TN, FP, FN):\n",
    "        return (TP+TN)/(TP+FP+FN+TN)\n",
    "\n",
    "    @classmethod\n",
    "    def recall(cls, TP, FN):\n",
    "        return TP/(TP+FN)\n",
    "\n",
    "    @classmethod\n",
    "    def precision(cls, TP, FP):\n",
    "        return TP/(TP+FP)\n",
    "\n",
    "    @classmethod\n",
    "    def f1(cls, recall, precision):\n",
    "        return 2*(recall*precision)/(recall+precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.classificator = Classificator()\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, filename):\n",
    "        msgs = {\n",
    "            'data' : [],\n",
    "            'target' : []\n",
    "        }\n",
    "\n",
    "        with open(filename) as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                target, msg = line.split('\\t')\n",
    "                msgs['data'].append(msg)\n",
    "                msgs['target'].append(target)\n",
    "        \n",
    "        return msgs\n",
    "\n",
    "    @classmethod\n",
    "    def preprocess(cls, X):\n",
    "        # lowercase (str -> str)\n",
    "        X = [line.lower() for line in X]\n",
    "\n",
    "        # numbers (str -> str)\n",
    "        X = [re.sub(r'\\d+', '', line) for line in X]\n",
    "\n",
    "        # punctuation (str -> str)\n",
    "        X = [line.translate(str.maketrans(\"\", \"\", string.punctuation)) for line in X]\n",
    "\n",
    "        #replace ’ with ' and remove “ and ” (str -> str)\n",
    "        X = [line.replace(\"’\",\"'\") for line in X]\n",
    "        X = [line.replace(\"“\",\"\") for line in X]\n",
    "        X = [line.replace(\"”\",\"\") for line in X]\n",
    "\n",
    "        # whitespaces (str -> str)\n",
    "        X = [line.strip() for line in X]\n",
    "\n",
    "        # tokenization (str -> list)\n",
    "        X = [line.split(' ') for line in X]\n",
    "\n",
    "        # stopwords (list -> list)\n",
    "        # stopwords = set(STOPWORDS)\n",
    "        # stopwords.update({})\n",
    "        # X = [[word for word in line if len(word) > 2 and not word in stopwords] for line in X]\n",
    "\n",
    "        # Lemmatization (list -> list)\n",
    "        X = [[WordNetLemmatizer().lemmatize(word) for word in line] for line in X]\n",
    "\n",
    "        return X\n",
    "    \n",
    "    @classmethod\n",
    "    def split(cls, data, target, test_size=0.3, random_state=109):\n",
    "        # Split dataset into training set and test set\n",
    "        # 70% training and 30% test (default)\n",
    "        return train_test_split(data, target, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    def train_test(self, filename=\"SMSSpamCollection\", test_size=0.3, random_state=109):\n",
    "        msgs = self.load(filename)\n",
    "        data = self.preprocess(msgs['data'])\n",
    "        X_train, X_test, Y_train, Y_test = self.split(data, msgs['target'], test_size, random_state)\n",
    "        self.classificator.train(X_train, Y_train)\n",
    "        return self.classificator.test(X_test, Y_test)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if isinstance(X, str):\n",
    "            X = [X]\n",
    "        data = self.preprocess(X)\n",
    "        return self.classificator.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(0.9533771667662881, 0.853448275862069, 0.8181818181818182, 0.8354430379746834)"
     },
     "metadata": {},
     "execution_count": 75
    }
   ],
   "source": [
    "model = Model()\n",
    "model.train_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['ham']"
     },
     "metadata": {},
     "execution_count": 65
    }
   ],
   "source": [
    "model.predict(\"I'm gonna be home soon and i don't want to talk about this stuff anymore tonight, k? I've cried enough today.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['spam']"
     },
     "metadata": {},
     "execution_count": 70
    }
   ],
   "source": [
    "model.predict(\"Congrats! Urgent! 1 year special cinema pass for 2 is yours.\")"
   ]
  }
 ]
}